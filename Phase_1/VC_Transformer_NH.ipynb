{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be38860a",
   "metadata": {},
   "source": [
    "# **Virtual Cell Challenge - Transformer NH**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597db0fd",
   "metadata": {},
   "source": [
    "### Set up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d92151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import anndata as ad\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4846fd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/____/Documents/VirtualCell'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set working directory \n",
    "os.chdir(\"/home/____/Documents/VirtualCell\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fc45e",
   "metadata": {},
   "source": [
    "### Preparing Training Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580cf8b",
   "metadata": {},
   "source": [
    "#### Obtain VCC training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bcde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read VCC data\n",
    "VCC_Training = sc.read_h5ad(\"adata_Training.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6457b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log1p normalization\n",
    "sc.pp.log1p(VCC_Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15de5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Control cells\n",
    "Control_Cells = VCC_Training[VCC_Training.obs['target_gene'] == 'non-targeting'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4d19c",
   "metadata": {},
   "source": [
    "## **Phase 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ee37",
   "metadata": {},
   "source": [
    "### Organize Inputs for Transformer\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8410c",
   "metadata": {},
   "source": [
    "#### Create vocab for GeneIDs and get tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0bae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of genes\n",
    "gene_list = list(VCC_Training.var_names)\n",
    "\n",
    "# Create gene to ID mapping\n",
    "gene_to_id = {gene: i for i, gene in enumerate(gene_list)}\n",
    "\n",
    "# Add a pad token \n",
    "gene_to_id['<pad>'] = len(gene_to_id)\n",
    "\n",
    "# Create tensor\n",
    "gene_ids = [gene_to_id[gene] for gene in gene_list]\n",
    "gene_ids_tensor = torch.tensor(gene_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9162eb",
   "metadata": {},
   "source": [
    "#### Create UMI tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d395562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UMI counts for each cell in control data\n",
    "umi_counts_control = np.array(Control_Cells.X.sum(axis=1)).flatten()\n",
    "\n",
    "# Log1p normalization\n",
    "log1p_umi_control = np.log1p(umi_counts_control)\n",
    "\n",
    "# Binning\n",
    "n_umi_bins = 32\n",
    "umi_bins = pd.cut(log1p_umi_control, bins=n_umi_bins, labels=False, include_lowest=True)\n",
    "binned_umi_tensor = torch.tensor(umi_bins, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c1b0a",
   "metadata": {},
   "source": [
    "#### Create Perturbation ID Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce7ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create perturb vocab and add 'non-targeting' token (control)\n",
    "perturb_vocab = gene_to_id.copy()\n",
    "perturb_vocab['non-targeting'] = len(perturb_vocab)\n",
    "\n",
    "vocab_size = len(perturb_vocab)\n",
    "\n",
    "# Create tensor of non targeting ID from vocab\n",
    "non_targeting_id = perturb_vocab['non-targeting']\n",
    "num_control_cells = Control_Cells.n_obs\n",
    "perturb_ids_tensor = torch.full((num_control_cells,), fill_value=non_targeting_id, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce22288",
   "metadata": {},
   "source": [
    "#### Create Expression Bins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a031af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_expression_bins = 51 # Number of bins for expression values\n",
    "\n",
    "# Get min and max of expression data to create edges for binning\n",
    "expression_data_binning = Control_Cells.X.toarray()\n",
    "non_zero_mask = expression_data_binning > 0\n",
    "non_zero_values = expression_data_binning[non_zero_mask]\n",
    "min_val, max_val = np.min(non_zero_values), np.max(non_zero_values)\n",
    "\n",
    "# Create bins\n",
    "bins = np.linspace(min_val, max_val, num=n_expression_bins)\n",
    "binned_data = np.zeros(expression_data_binning.shape, dtype=np.int32)\n",
    "binned_data[non_zero_mask] = np.digitize(non_zero_values, bins=bins)\n",
    "Control_Cells.layers['binned'] = binned_data\n",
    "\n",
    "# Pad token for masking\n",
    "pad_token_id = n_expression_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1893a51",
   "metadata": {},
   "source": [
    "#### Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37412710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Create a class for the dataset\n",
    "class VCC_Training_Dataset_Phase1(Dataset):\n",
    "    def __init__(self, adata, gene_ids_tensor, binned_umi_tensor, perturb_ids_tensor, mask_prob=0.15, pad_token_id = n_expression_bins):\n",
    "        self.X_binned = adata.layers['binned']\n",
    "        self.X_true = adata.X.toarray()\n",
    "        self.gene_ids_tensor = gene_ids_tensor\n",
    "        self.binned_umi_tensor = binned_umi_tensor\n",
    "        self.perturb_ids_tensor = perturb_ids_tensor\n",
    "        self.mask_prob = mask_prob\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_binned.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        binned_expr = torch.tensor(self.X_binned[idx, :], dtype = torch.long)\n",
    "        true_expr = torch.tensor(self.X_true[idx, :], dtype = torch.float)\n",
    "\n",
    "        masked_binned_expr = binned_expr.clone()\n",
    "        mask = torch.rand(binned_expr.shape) < self.mask_prob\n",
    "        masked_binned_expr[mask] = self.pad_token_id\n",
    "\n",
    "        return {\"gene_ids\": self.gene_ids_tensor,\n",
    "                \"binned_expression_masked\": masked_binned_expr,\n",
    "                \"true_expression\": true_expr,\n",
    "                \"binned_umi\": self.binned_umi_tensor[idx],\n",
    "                \"perturb_id\": self.perturb_ids_tensor[idx],\n",
    "                \"mask\": mask\n",
    "                }\n",
    "\n",
    "# Instantiate the Phase 1 dataset\n",
    "Phase1_Dataset = VCC_Training_Dataset_Phase1(Control_Cells, gene_ids_tensor, binned_umi_tensor, perturb_ids_tensor)\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(Phase1_Dataset))\n",
    "val_size = len(Phase1_Dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(Phase1_Dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = 1, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e089cb",
   "metadata": {},
   "source": [
    "### Constructing Transformer Architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8d035",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e9e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Class\n",
    "class VCC_Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_expression_bins, n_umi_bins, embedding_dim, n_heads, n_layers, pad_token_id):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding Layers\n",
    "        self.shared_embedding = nn.Embedding(vocab_size, embedding_dim) \n",
    "        self.expression_embedding = nn.Embedding(n_expression_bins + 1, embedding_dim, padding_idx=pad_token_id)\n",
    "        self.umi_embedding = nn.Embedding(n_umi_bins, embedding_dim)\n",
    "        self.perturb_transform = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.embedding_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.embedding_dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = n_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Output Prediction\n",
    "        self.prediction_head = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        gene_ids = batch['gene_ids']\n",
    "        binned_expression = batch['binned_expression_masked']\n",
    "        umi_bin = batch['binned_umi']\n",
    "        perturb_id = batch['perturb_id']\n",
    "\n",
    "        # Create Embeddings\n",
    "        gene_embed = self.shared_embedding(gene_ids)\n",
    "        expr_embed = self.expression_embedding(binned_expression)\n",
    "        sequence_embed = gene_embed + expr_embed\n",
    "        sequence_embed = self.embedding_norm(sequence_embed)\n",
    "        sequence_embed = self.embedding_dropout(sequence_embed)\n",
    "        umi_embed = self.umi_embedding(umi_bin).unsqueeze(1)\n",
    "        initial_p_embed = self.shared_embedding(perturb_id).unsqueeze(1)\n",
    "        final_p_embed = self.perturb_transform(initial_p_embed)\n",
    "\n",
    "        # Assemble and Process the Sequence\n",
    "        full_sequence = torch.cat([umi_embed, final_p_embed, sequence_embed], dim=1)\n",
    "\n",
    "        # Pass Sequence through Tranformer\n",
    "        transformer_output = self.transformer_encoder(full_sequence)\n",
    "\n",
    "        # Keep only Gene ouputs\n",
    "        gene_outputs = transformer_output[:, 2:, :] \n",
    "\n",
    "        # Pass through prediction head\n",
    "        predictions = self.prediction_head(gene_outputs)\n",
    "\n",
    "        return predictions.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790455c",
   "metadata": {},
   "source": [
    "#### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2211b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM  = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 6\n",
    "\n",
    "# Instantiate Model\n",
    "model = VCC_Transformer(vocab_size=vocab_size,n_expression_bins=n_expression_bins, n_umi_bins=n_umi_bins, embedding_dim=EMBED_DIM, n_heads=N_HEADS, n_layers=N_LAYERS, pad_token_id=pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5d75f",
   "metadata": {},
   "source": [
    "### Running Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39483384",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd53444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Setup loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f85ff",
   "metadata": {},
   "source": [
    "#### Training and Validation Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa08e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # --- Training Phase\n",
    "\n",
    "    # Setup\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Training batch loop\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for batch in train_pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward Pass\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(batch)\n",
    "            true_expr = batch['true_expression']\n",
    "            mask = batch['mask']\n",
    "\n",
    "            loss = criterion(predictions[mask], true_expr[mask])\n",
    "\n",
    "        # Backward Pass\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation Phase\n",
    "\n",
    "    # Setup\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_pbar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                predictions = model(batch)\n",
    "                true_expr = batch['true_expression']\n",
    "                mask = batch['mask']\n",
    "                \n",
    "                loss = criterion(predictions[mask], true_expr[mask])\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
