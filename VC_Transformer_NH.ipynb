{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be38860a",
   "metadata": {},
   "source": [
    "# **Virtual Cell Challenge - Transformer NH**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597db0fd",
   "metadata": {},
   "source": [
    "### Set up\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d92151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import torch \n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import anndata as ad\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4846fd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nico/Documents/VirtualCell'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set working directory \n",
    "os.chdir(\"/home/nico/Documents/VirtualCell\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170fc45e",
   "metadata": {},
   "source": [
    "### Preparing Training Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580cf8b",
   "metadata": {},
   "source": [
    "#### Obtain VCC training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bcde7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read VCC data\n",
    "VCC_Training = sc.read_h5ad(\"adata_Training.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6457b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log1p normalization\n",
    "sc.pp.log1p(VCC_Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15de5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Control cells\n",
    "Control_Cells = VCC_Training[VCC_Training.obs['target_gene'] == 'non-targeting'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4d19c",
   "metadata": {},
   "source": [
    "## **Phase 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ee37",
   "metadata": {},
   "source": [
    "### Organize Inputs for Transformer\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f8410c",
   "metadata": {},
   "source": [
    "#### Create vocab for GeneIDs and get tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0bae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of genes\n",
    "gene_list = list(VCC_Training.var_names)\n",
    "\n",
    "# Create gene to ID mapping\n",
    "gene_to_id = {gene: i for i, gene in enumerate(gene_list)}\n",
    "\n",
    "# Add a pad token \n",
    "gene_to_id['<pad>'] = len(gene_to_id)\n",
    "\n",
    "# Create tensor\n",
    "gene_ids = [gene_to_id[gene] for gene in gene_list]\n",
    "gene_ids_tensor = torch.tensor(gene_ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9162eb",
   "metadata": {},
   "source": [
    "#### Create UMI tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d395562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get UMI counts for each cell in control data\n",
    "umi_counts_control = np.array(Control_Cells.X.sum(axis=1)).flatten()\n",
    "\n",
    "# Log1p normalization\n",
    "log1p_umi_control = np.log1p(umi_counts_control)\n",
    "\n",
    "# Binning\n",
    "n_umi_bins = 32\n",
    "umi_bins = pd.cut(log1p_umi_control, bins=n_umi_bins, labels=False, include_lowest=True)\n",
    "binned_umi_tensor = torch.tensor(umi_bins, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c1b0a",
   "metadata": {},
   "source": [
    "#### Create Perturbation ID Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ce7ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create perturb vocab and add 'non-targeting' token (control)\n",
    "perturb_vocab = gene_to_id.copy()\n",
    "perturb_vocab['non-targeting'] = len(perturb_vocab)\n",
    "\n",
    "vocab_size = len(perturb_vocab)\n",
    "\n",
    "# Create tensor of non targeting ID from vocab\n",
    "non_targeting_id = perturb_vocab['non-targeting']\n",
    "num_control_cells = Control_Cells.n_obs\n",
    "perturb_ids_tensor = torch.full((num_control_cells,), fill_value=non_targeting_id, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce22288",
   "metadata": {},
   "source": [
    "#### Create Expression Bins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a031af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_expression_bins = 51 # Number of bins for expression values\n",
    "\n",
    "# Get min and max of expression data to create edges for binning\n",
    "expression_data_binning = Control_Cells.X.toarray()\n",
    "non_zero_mask = expression_data_binning > 0\n",
    "non_zero_values = expression_data_binning[non_zero_mask]\n",
    "min_val, max_val = np.min(non_zero_values), np.max(non_zero_values)\n",
    "\n",
    "# Create bins\n",
    "bins = np.linspace(min_val, max_val, num=n_expression_bins)\n",
    "binned_data = np.zeros(expression_data_binning.shape, dtype=np.int32)\n",
    "binned_data[non_zero_mask] = np.digitize(non_zero_values, bins=bins)\n",
    "Control_Cells.layers['binned'] = binned_data\n",
    "\n",
    "# Pad token for masking\n",
    "pad_token_id = n_expression_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1893a51",
   "metadata": {},
   "source": [
    "#### Construct Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37412710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Create a class for the dataset\n",
    "class VCC_Training_Dataset_Phase1(Dataset):\n",
    "    def __init__(self, adata, gene_ids_tensor, binned_umi_tensor, perturb_ids_tensor, mask_prob=0.15, pad_token_id = n_expression_bins):\n",
    "        self.X_binned = adata.layers['binned']\n",
    "        self.X_true = adata.X.toarray()\n",
    "        self.gene_ids_tensor = gene_ids_tensor\n",
    "        self.binned_umi_tensor = binned_umi_tensor\n",
    "        self.perturb_ids_tensor = perturb_ids_tensor\n",
    "        self.mask_prob = mask_prob\n",
    "        self.pad_token_id = pad_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_binned.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        binned_expr = torch.tensor(self.X_binned[idx, :], dtype = torch.long)\n",
    "        true_expr = torch.tensor(self.X_true[idx, :], dtype = torch.float)\n",
    "\n",
    "        masked_binned_expr = binned_expr.clone()\n",
    "        mask = torch.rand(binned_expr.shape) < self.mask_prob\n",
    "        masked_binned_expr[mask] = self.pad_token_id\n",
    "\n",
    "        return {\"gene_ids\": self.gene_ids_tensor,\n",
    "                \"binned_expression_masked\": masked_binned_expr,\n",
    "                \"true_expression\": true_expr,\n",
    "                \"binned_umi\": self.binned_umi_tensor[idx],\n",
    "                \"perturb_id\": self.perturb_ids_tensor[idx],\n",
    "                \"mask\": mask\n",
    "                }\n",
    "\n",
    "# Instantiate the Phase 1 dataset\n",
    "Phase1_Dataset = VCC_Training_Dataset_Phase1(Control_Cells, gene_ids_tensor, binned_umi_tensor, perturb_ids_tensor)\n",
    "\n",
    "# Split data\n",
    "train_size = int(0.8 * len(Phase1_Dataset))\n",
    "val_size = len(Phase1_Dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(Phase1_Dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size = 1, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e089cb",
   "metadata": {},
   "source": [
    "### Constructing Transformer Architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8d035",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e9e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Class\n",
    "class VCC_Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, n_expression_bins, n_umi_bins, embedding_dim, n_heads, n_layers, pad_token_id):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding Layers\n",
    "        self.shared_embedding = nn.Embedding(vocab_size, embedding_dim) \n",
    "        self.expression_embedding = nn.Embedding(n_expression_bins + 1, embedding_dim, padding_idx=pad_token_id)\n",
    "        self.umi_embedding = nn.Embedding(n_umi_bins, embedding_dim)\n",
    "        self.perturb_transform = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.embedding_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.embedding_dropout = nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model = embedding_dim, nhead = n_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Output Prediction\n",
    "        self.prediction_head = nn.Linear(embedding_dim, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        gene_ids = batch['gene_ids']\n",
    "        binned_expression = batch['binned_expression_masked']\n",
    "        umi_bin = batch['binned_umi']\n",
    "        perturb_id = batch['perturb_id']\n",
    "\n",
    "        # Create Embeddings\n",
    "        gene_embed = self.shared_embedding(gene_ids)\n",
    "        expr_embed = self.expression_embedding(binned_expression)\n",
    "        sequence_embed = gene_embed + expr_embed\n",
    "        sequence_embed = self.embedding_norm(sequence_embed)\n",
    "        sequence_embed = self.embedding_dropout(sequence_embed)\n",
    "        umi_embed = self.umi_embedding(umi_bin).unsqueeze(1)\n",
    "        initial_p_embed = self.shared_embedding(perturb_id).unsqueeze(1)\n",
    "        final_p_embed = self.perturb_transform(initial_p_embed)\n",
    "\n",
    "        # Assemble and Process the Sequence\n",
    "        full_sequence = torch.cat([umi_embed, final_p_embed, sequence_embed], dim=1)\n",
    "\n",
    "        # Pass Sequence through Tranformer\n",
    "        transformer_output = self.transformer_encoder(full_sequence)\n",
    "\n",
    "        # Keep only Gene ouputs\n",
    "        gene_outputs = transformer_output[:, 2:, :] \n",
    "\n",
    "        # Pass through prediction head\n",
    "        predictions = self.prediction_head(gene_outputs)\n",
    "\n",
    "        return predictions.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4790455c",
   "metadata": {},
   "source": [
    "#### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2211b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM  = 256\n",
    "N_HEADS = 8\n",
    "N_LAYERS = 6\n",
    "\n",
    "# Instantiate Model\n",
    "model = VCC_Transformer(vocab_size=vocab_size,n_expression_bins=n_expression_bins, n_umi_bins=n_umi_bins, embedding_dim=EMBED_DIM, n_heads=N_HEADS, n_layers=N_LAYERS, pad_token_id=pad_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5d75f",
   "metadata": {},
   "source": [
    "### Running Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39483384",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd53444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# Setup loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168f85ff",
   "metadata": {},
   "source": [
    "#### Training and Validation Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa08e644",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]:   1%|          | 177/30540 [04:52<13:56:10,  1.65s/it, loss=0.828]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 25\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     26\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     28\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/amp/grad_scaler.py:453\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    451\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 453\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    455\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/amp/grad_scaler.py:350\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/amp/grad_scaler.py:350\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # --- Training Phase\n",
    "\n",
    "    # Setup\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Training batch loop\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for batch in train_pbar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        # Forward Pass\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(batch)\n",
    "            true_expr = batch['true_expression']\n",
    "            mask = batch['mask']\n",
    "\n",
    "            loss = criterion(predictions[mask], true_expr[mask])\n",
    "\n",
    "        # Backward Pass\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # --- Validation Phase\n",
    "\n",
    "    # Setup\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_pbar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                predictions = model(batch)\n",
    "                true_expr = batch['true_expression']\n",
    "                mask = batch['mask']\n",
    "                \n",
    "                loss = criterion(predictions[mask], true_expr[mask])\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            val_pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e15392f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VCC_Transformer(\n",
       "  (shared_embedding): Embedding(18082, 256)\n",
       "  (expression_embedding): Embedding(52, 256, padding_idx=51)\n",
       "  (umi_embedding): Embedding(32, 256)\n",
       "  (perturb_transform): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (embedding_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (prediction_head): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
